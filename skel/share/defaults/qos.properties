#  -----------------------------------------------------------------------
#     Default values for qos
#
#     The qos services are responsible for maintaining the disk and tape
#     requirements of a given file.
#  -----------------------------------------------------------------------
@DEFAULTS_HEADER@

#  ---- Cell names of qos services
#
qos.cell.name=qos
qos.engine.cell.name=qos-engine
qos.verifier.cell.name=qos-verifier
qos.scanner.cell.name=qos-scanner
qos.adjuster.cell.name=qos-adjuster

#  ---- Named queues to consume from
#
#       A service can consume messages from named queues. Other services can
#       write messages to such queues. A named queue has an unqualified cell
#       address, that is, an address without a domain name.
#
#       This property contains a comma separated list of named queues to
#       consume from.
#
qos.cell.consume = ${qos.cell.name}
qos.engine.cell.consume = ${qos.engine.cell.name}
qos.verifier.cell.consume = ${qos.verifier.cell.name}
qos.scanner.cell.consume = ${qos.scanner.cell.name}
qos.adjuster.cell.consume = ${qos.adjuster.cell.name}

#  ---- Message topics to subscribe to.
#
qos.cell.subscribe=${qos.cache-location-topic},\
  ${qos.corrupt-file-topic},\
  ${qos.pool-monitor-topic}

qos.engine.cell.subscribe=${qos.cache-location-topic},\
  ${qos.corrupt-file-topic},\
  ${qos.pool-monitor-topic}

qos.verifier.cell.subscribe=${qos.pool-monitor-topic}

qos.scanner.cell.subscribe=${qos.pool-monitor-topic}

# ---- Listens for location updates from PnfsManager.
#
qos.cache-location-topic=CacheLocationTopic

# ---- Listens for checksum scanner or pool reports.  If the corrupt
#      file is a non-unique replica, it tries to handle this by removing
#      the copy and making a new one.
#
qos.corrupt-file-topic=${dcache.corrupt-file.topic}

# ---- Channel on which pool monitor updates are pushed out.
#      Resilience relies on these for current info regarding pools,
#      pool groups, storage units, pool mode/status, pool tags, and pool cost.
#
qos.pool-monitor-topic=${dcache.pool-monitor.topic}

# ---- Publishes transition completed messages on this topic.
#
qos.transition-completed-topic=${dcache.qos-transition.topic}

# ---- Base directory where any qos metadata is stored.  This
#      includes the checkpoint file, inaccessible file lists, and statistics
#      output.
#
qos.home=${dcache.paths.qos}

# ---- Configuration for database connection pool ---------------------------
#
#      The database connection pool reuses connections between successive
#      database operations.  By reusing connections dCache doesn't suffer
#      the overhead of establishing new database connections for each
#      operation.
#
#      The options here determine how qos behaves as the number of concurrent
#      requests fluctuates.
# ---------------------------------------------------------------------------

# ---- The maximum number of concurrent database connections
#
#      The recommended minimum setting is the number of scan threads
#      plus a few more for admin calls.
#
#      Since the scanner service shares the chimera database with pnfsmanager,
#      be sure to adjust the postgresql.conf max connections upwards
#      to accommodate both.  Pnfsmanager runs well with about 100
#      connections.  Adding a separate qos service means the
#      connections should be increased by at least the amount below.
#
qos.db.connections.max=10

# ---- The minimum number of idle database connections.
#
qos.db.connections.idle=1

(prefix)qos.db.hikari-properties = Hikari-specific properties

# ---- Database related settings reserved for internal use.
#
(immutable)qos.db.host=${chimera.db.host}
(immutable)qos.db.name=${chimera.db.name}
(immutable)qos.db.user=${chimera.db.user}
(immutable)qos.db.password=${chimera.db.password}
(immutable)qos.db.password.file=${chimera.db.password.file}
(immutable)qos.db.url=${chimera.db.url}
(immutable)qos.db.schema.changelog=${chimera.db.schema.changelog}
(immutable)qos.db.schema.auto=false

# ---- Used with the pool scan query. This is a hint given to the jdbc driver
#      to decrease the number of round-trips to the database on large result
#      sets (by default it is 0, meaning ignored).  Setting this too high
#      may, however, adversely affect performance.
#
qos.db.fetch-size=1000

# ---- Replace with org.dcache.chimera.namespace.ChimeraEnstoreStorageInfoExtractor
#      if you are running an enstore HSM backend.
#
qos.plugins.storage-info-extractor=${dcache.plugins.storage-info-extractor}

# ---- Checkpointing.
#
#      How often the verification operation table is to be saved to disk for
#      the purposes of recovery.
#
qos.limits.checkpoint-expiry=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.checkpoint-expiry.unit=MINUTES

# ---- Thread queues --------------------------------------------------------------
#
#      There are different thread queues associated with each of the qos services.
#
#      In general, each (remote) service has an executor for handling the
#      processing of incoming messages.  The thread pools for these
#      are labeled 'submit-threads.'  In the case of the verifier,
#      there is also a bulk submission pool for handling bulk scan requests.
#
#      The verifier, scanner and adjuster in addition also have task thread pools.
# ---------------------------------------------------------------------------------

# ---- Thread queue used during the request for and update to requirements.
#      In the current implementation, each thread makes a call to the namespace.
#
qos.limits.requirements.submit-threads=32

# ---- Thread queue used during the registration of a new operation.
#      Updates will do a verification of the pool locations; cancellation
#      also runs on these threads, but involves simply setting up a filter
#      (the actual cancellation is run on a different thread).
#
qos.limits.verifier.submit-threads=32

# ---- Thread queue used during the registration of a new operation.
#      Like the above, but loops over a list of files.
#
qos.limits.verifier.bulk-threads=8

# ---- Thread queue used when an operation becomes active (to verify
#      the requirements and send a message to the adjuster). Each thread makes
#      a call to the requirements service, then to the pools, and finally,
#      if necessary, to the adjuster service.
#
qos.limits.verifier.task-threads=32

# ---- Thread queue used during the registration of a new adjustment task.
#      Minimal work is done on this thread.
#
qos.limits.adjuster.submit-threads=16

# ---- Thread queue used for tasks.  Note that the longer-running tasks like
#      like migration and staging relinquish the thread by waiting.
#
qos.limits.adjuster.task-threads=100

# ---- Thread queue used to handle responses from the verifier.  These
#      involve batched counts, and the amount of update work done on the
#      thread is small.  Should mirror the bulk threads on the verifier.
#
qos.limits.scanner.submit-threads=8

# ---- Thread queue used for scanning the namespace on pool state changes or
#      as part of a periodic check.  Requires a database connection,
#      which it holds onto for the life of the task being executed.
#
#      A note on pool operation throttling:
#
#      A pool scan or processing of a pool status message can generate
#      thousands, even millions, of file tasks.  Allowing too many pool
#      operations to run simultaneously can, aside from the increased
#      pressure on the namespace database, potentially overload the system.
#      Lowering the number of available threads may be necessary
#      if the number of files per pool is on the order of 2 million or
#      greater (or, alternately, one may need to increase the memory of the
#      JVM for the scanner service).
#
qos.limits.scanner.task-threads=5

qos.limits.scanner.pool-op-init-grace-period=5
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.pool-op-init-grace-period.unit=MINUTES

# ---- The number of verification operations needs to be bounded so as not
#      to flood either the adjuster service or the pin manager (for staging).
#      This number need not, however, reflect the number of available task threads
#      in the adjuster itself.
#
#      A note on the memory footprint.  Each verification operation entry
#      requires 128 bytes minimum (assuming 8-byte alignment) of primitives or
#      object references. If we assume each reference could encompass 512 bytes,
#      then 10 million entries would require about 5 GB.  However, it is
#      advisable to set the verifier domain heap size to at least 8 GB,
#      because there is other overhead during normal processing which
#      can drive up peak usage values.  Naturally, if the service runs
#      together with other services in the JVM, the heap size should also be
#      adjusted upward.
#
(immutable)qos.limits.verifier.max-running-operations=200

# ---- So that long-lived staging requests do no starve out other kinds of
#      requests, we only allow this proportion of concurrently running operations
#      to be staging.
qos.limits.verifier.staging-max-allocation=0.5

# ---- Size of buffer for displaying history of the most
#      recently completed tasks and operations.
#
qos.limits.adjuster.task-history=1000
qos.limits.verifier.operation-history=1000

# ---- Retry management.
#
#      The following property controls the number of
#      times the verifier is allowed to retry failed file-operations.
#      This is on a per-source/target basis, if the error is judged retriable.
#      If there is a non-retriable error, but a different source or target
#      can be selected, the retry count is set back to 0 again.
#
qos.limits.verifier.operation-retries=1

# ---- Retry management.
#
#      The following property controls the number of
#      times the adjuster is allowed to retry failed adjustment tasks.
#
qos.limits.adjuster.task-retries=1

# ---- Operation and task map checking.
#
#      The maximum interval which can pass before a check of waiting/completed
#      operations or tasks is run (for an active system the interval will effectively
#      be shorter, as checks are also done each time a running task terminates).
#
qos.limits.verifier.scan-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verifier.scan-period.unit=MINUTES
qos.limits.adjuster.scan-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.adjuster.scan-period.unit=MINUTES

# ---- Pool manager pool info refreshing.
#
#      Information concerning pool cost is considered out of sync after
#      this interval has passed.   This should be somewhat longer than
#      the notification period value(see poolmanager.pool-monitor.update-period and
#      poolmanager.pool-monitor.update-period.unit).
#
qos.limits.pool-info-expiry=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.pool-info-expiry.unit=MINUTES

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool down update
#      and actually launching a scan operation to check replicas on
#      that pool.  Setting to 0 will trigger the scan immediately.
#
qos.limits.scanner.down-grace-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.down-grace-period.unit=HOURS

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool restart update
#      and actually launching a scan operation to check replicas on
#      that pool. Setting to 0 will trigger the scan immediately.
#
qos.limits.scanner.restart-grace-period=6
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.restart-grace-period.unit=HOURS

# ---- Startup
#
#      When an entire dcache installation is brought on line at the same time,
#      pool status may not yet be available from the pool manager.  This
#      property sets an initial delay before pool info initialization
#      begins.  Setting this property to 0 skips the delay.
#
qos.limits.startup-delay=30
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.startup-delay.unit=SECONDS

# ---- Operation launch
#
#      Scheduling the operation can include a delay between when it has been selected to run
#      and when it actually starts to execute.  This may be useful in some situations
#      to avoid races between PnfsManager and qos in terms of the initial
#      write to a pool.  This property can also be configured from the admin
#      interface using file ctrl reset.
#
qos.limits.verification.launch-delay=0
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verification.launch-delay.unit=SECONDS

# ---- Copy/migration target selection.
#
#      Strategy implementation used to select among available/eligible target pools.
#
qos.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy

# ---- Periodic scanning (watchdog).
#
#      The following properties control the periodic scanning of pools to check
#      for qos consistency and initiate any adjustments that may be necessary
#      in the case of inconsistent state.  The scan period refers to the default
#      amount of time between sweeps of the pools (absent pool status change events).
#      The scan window refers to the maximum amount of time that can elapse since
#      a pool was scanned after which another scan of that specific pool will be
#      initiated. Disabling the watchdog means the system will only respond to actual
#      ool state change events, but will not automatically scan pools after the window
#      period has elapsed.
#
(one-of?true|false)qos.enable.watchdog=true
qos.watchdog.scan.period=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.watchdog.scan.period.unit=MINUTES
qos.watchdog.scan.window=24
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.watchdog.scan.window.unit=HOURS

# ---- Endpoint (cell) settings for contacting pin manager.
#
qos.service.pinmanager=${dcache.service.pinmanager}
qos.service.pinmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pinmanager.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for contacting pnfs manager.
#
qos.service.pnfsmanager=${dcache.service.pnfsmanager}
qos.service.pnfsmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pnfsmanager.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for contacting pools (destination is dynamic).
#
qos.service.pool.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pool.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for the qos.transition-completed-topic.
#
qos.service.transition.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.transition.timeout.unit=MINUTES

# ---- Main external entry point for qos.
#
qos.service.requirements=${dcache.service.qos}
qos.service.requirements.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.requirements.timeout.unit=MINUTES

# ---- Internal endpoints consumed only by other qos services.
#
(immutable)qos.service.adjustment=${qos.adjuster.cell.name}
qos.service.adjustment.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.adjustment.timeout.unit=MINUTES

(immutable)qos.service.scanner=${qos.scanner.cell.name}
qos.service.scanner.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.scanner.timeout.unit=MINUTES

(immutable)qos.service.verification=${qos.verifier.cell.name}
qos.service.verification.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.verification.timeout.unit=MINUTES
